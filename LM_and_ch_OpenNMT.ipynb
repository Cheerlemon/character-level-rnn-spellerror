{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import math\n",
    "\n",
    "API_URL = \"http://api.netspeak.org/netspeak3/search?query=%s\"\n",
    "\n",
    "class NetSpeak:\n",
    "    def __init__(self):\n",
    "        self.headers = {'User-Agent': 'Mozilla/5.0 (compatible; MSIE 5.5; Windows NT)'}\n",
    "        self.page = None\n",
    "        self.dictionary = {}\n",
    "\n",
    "    def __getPageContent(self, url):\n",
    "        return requests.get(url, headers=self.headers).text\n",
    "        # return self.opener.open(url).read()\n",
    "\n",
    "    def __rolling(self, url, maxfreq=None):\n",
    "        if maxfreq:\n",
    "            webdata = self.__getPageContent(url + \"&maxfreq=%s\" % maxfreq)\n",
    "        else:\n",
    "            webdata = self.__getPageContent(url)\n",
    "        if webdata:\n",
    "            # webdata = webdata.decode('utf-8')\n",
    "            results = [data.split('\\t') for data in webdata.splitlines()]\n",
    "            results = [(data[2], float(data[1])) for data in results]\n",
    "            lastFreq = int(results[-1][1])\n",
    "            if lastFreq != maxfreq:\n",
    "                return results + self.__rolling(url, lastFreq)\n",
    "            else:\n",
    "                return []\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def search(self, query):\n",
    "        if query in self.dictionary: return self.dictionary[query]\n",
    "        \n",
    "        queries = query.lower().split()\n",
    "        new_query = []\n",
    "        for token in queries:\n",
    "            if token.count('|') > 0:\n",
    "                new_query.append('[+{0}+]'.format('+'.join(token.split('|'))))\n",
    "            elif token == '*':\n",
    "                new_query.append('?')\n",
    "            else:\n",
    "                new_query.append(token)\n",
    "        new_query = '+'.join(new_query)\n",
    "        url = API_URL % (new_query.replace(' ', '+'))\n",
    "        self.dictionary[query] = self.__rolling(url)\n",
    "        return self.dictionary[query]\n",
    "    \n",
    "SE = NetSpeak() # singleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "confuse_word = open('lab4.confusables.txt','r').readlines()\n",
    "Confuse = {}\n",
    "for line in confuse_word:\n",
    "    w ,c = line.split('\\t')\n",
    "    Confuse[w]=c.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigrams(tokens):\n",
    "    return [tokens[i:i+3] for i in range(len(tokens) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_where(tm):\n",
    "    trigrams = get_trigrams(tm)\n",
    "    tri_tmp = []\n",
    "    for index,tri in enumerate(trigrams):\n",
    "        #print(tri)\n",
    "        res = SE.search(' '.join(tri))\n",
    "        #print(res)\n",
    "        if res:\n",
    "            tri_tmp.append((index,res[0][1],tri))\n",
    "        else:\n",
    "            tri_tmp.append((index,0,tri))\n",
    "    #print(tri_tmp)\n",
    "    minn  = min(tri_tmp,key=lambda x:x[1])[2]\n",
    "    #print(minn)\n",
    "    for find_index in tri_tmp:\n",
    "        #print(find_index[2])\n",
    "        if find_index[2]==minn:\n",
    "            detect_sentence = find_index\n",
    "            \n",
    "    return detect_sentence\n",
    "\n",
    "def find_the_best(tm,start):\n",
    "    \n",
    "    best = (None, None, None, None, -math.inf)\n",
    "    #find_the_best = []\n",
    "    for i in range(start,start+3):\n",
    "        candidate = []\n",
    "        for corr in correction(tm[i]):\n",
    "            candidate.append(corr[0])\n",
    "        if tm[i] in Confuse.keys():\n",
    "            candidate.append(tm[i])\n",
    "        #print(candidate)\n",
    "        for cancan in candidate:\n",
    "            count = 1.0\n",
    "            combine = tm[:i] + [cancan] + tm[i+1:]\n",
    "            #print(combine)\n",
    "            trigrams = get_trigrams(combine)\n",
    "            \n",
    "            for tri in trigrams:\n",
    "                res = SE.search(' '.join(tri))\n",
    "                count *= res[0][1] if res else 0\n",
    "                #print(res,count)\n",
    "                \n",
    "            best = (combine,tm[i],cancan,candidate,count) if count > best[-1] else best\n",
    "       \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model parameters.\n"
     ]
    }
   ],
   "source": [
    "import onmt\n",
    "import onmt.io\n",
    "import onmt.translate\n",
    "import onmt.ModelConstructor\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "# Load the model.\n",
    "Opt = namedtuple('Opt', ['model', 'data_type', 'reuse_copy_attn', \"gpu\"])\n",
    "opt = Opt(\"ch-OpenNMT-py/ch-merge-model/demo_model_acc_91.31_ppl_1.70_e13.pt\", \"text\",False, 0)\n",
    "fields, model, model_opt =  onmt.ModelConstructor.load_test_model(opt,{\"reuse_copy_attn\":False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ch_OpenNMT_generate_candidate(detect_sentence_arr):\n",
    "    ch_candidate = {}\n",
    "    \n",
    "    f = open('text.txt','w')\n",
    "    for i in detect_sentence_arr:\n",
    "        for ii in i:\n",
    "            f.write(ii+' ')\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "    \n",
    "    data = onmt.io.build_dataset(fields, \"text\", \"text.txt\", None, use_filter_pred=False)\n",
    "    data_iter = onmt.io.OrderedIterator(\n",
    "        dataset=data, device=0,\n",
    "        batch_size=1, train=False, sort=False,\n",
    "        sort_within_batch=True, shuffle=False)\n",
    "    # Translator\n",
    "    scorer = onmt.translate.GNMTGlobalScorer(None,\n",
    "                                             None,\n",
    "                                             None,\n",
    "                                             None)\n",
    "    # Translator\n",
    "    translator = onmt.translate.Translator(model, fields,\n",
    "                                               beam_size=10,\n",
    "                                               n_best=5,\n",
    "                                               global_scorer=scorer,\n",
    "                                               cuda=True)\n",
    "    builder = onmt.translate.TranslationBuilder(\n",
    "            data, translator.fields,\n",
    "            5, False, None)\n",
    "    # Translator\n",
    "    scorer = onmt.translate.GNMTGlobalScorer(None,\n",
    "                                             None,\n",
    "                                             None,\n",
    "                                             None)\n",
    "    \n",
    "    translator = onmt.translate.Translator(model, fields,\n",
    "                                               beam_size=20,\n",
    "                                               n_best=10,\n",
    "                                               global_scorer=scorer,\n",
    "                                               cuda=True)\n",
    "    builder = onmt.translate.TranslationBuilder(\n",
    "            data, translator.fields,\n",
    "            10, False, None)\n",
    "    for batch in data_iter:\n",
    "        batch_data = translator.translate_batch(batch, data)\n",
    "        translations = builder.from_batch(batch_data)\n",
    "        for trans in translations:\n",
    "            n_best_preds = [\" \".join(pred) for pred in trans.pred_sents[:10]]\n",
    "        \n",
    "        ch_candidate[' '.join(translations[0].src_raw).replace(' ','')] = n_best_preds\n",
    "    \n",
    "    \n",
    "    return ch_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_best(tm,start):\n",
    "    best = (None, None, None, None, -math.inf)\n",
    "    #find_the_best = []\n",
    "    for i in range(start,start+3):\n",
    "        candidate = []\n",
    "        candidate = ch_candidate[word[i]]\n",
    "        if tm[i] in Confuse.keys():\n",
    "            candidate.append(tm[i])\n",
    "        #print(candidate)\n",
    "        for cancan in candidate:\n",
    "            count = 1.0\n",
    "            combine = tm[:i] + [cancan] + tm[i+1:]\n",
    "            #print(combine)\n",
    "            trigrams = get_trigrams(combine)\n",
    "            \n",
    "            for tri in trigrams:\n",
    "                res = SE.search(' '.join(tri))\n",
    "                count *= res[0][1] if res else 0\n",
    "                #print(res,count)\n",
    "                \n",
    "            best = (combine,tm[i],cancan,candidate,count) if count > best[-1] else best\n",
    "       \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分割正確跟錯誤的資料集\n",
    "line = open('lab4.test.1.txt','r').readlines()\n",
    "Correct_sentence = []\n",
    "False_sentence = []\n",
    "for sentence in line:\n",
    "    tmp = sentence.split('\\t')\n",
    "    False_sentence.append(tmp[0].strip().lower())\n",
    "    Correct_sentence.append(tmp[1].strip().lower())\n",
    "test_Correct=Correct_sentence[:20]\n",
    "test_False = False_sentence[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average src size 4.666666666666667 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlplab/ting/character-level-rnn-spellerror/onmt/modules/GlobalAttention.py:176: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  align_vectors = self.sm(align.view(batch*targetL, sourceL))\n",
      "/home/nlplab/ting/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "/home/nlplab/ting/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
     ]
    }
   ],
   "source": [
    "hits = 0\n",
    "for i,line in enumerate (test_False):\n",
    "    word = line.split(' ')\n",
    "    detect_sentence = detect_where(word)\n",
    "    start = detect_sentence[0]\n",
    "    ch_candidate = ch_OpenNMT_generate_candidate(detect_sentence[2])\n",
    "    combine ,wrong ,right ,candidate ,_ =find_the_best(word,start)\n",
    "    combine = ' '.join(combine).strip()\n",
    "    if combine == test_Correct[i]:\n",
    "        hits+=1\n",
    "        \n",
    "    print(\"Error:\" +  str(wrong))\n",
    "    print(\"Candidates:\", candidate)\n",
    "    print(\"Correction:\", right)\n",
    "    print(test_False[i], \"->\", combine )\n",
    "    print(\"hits =\", hits)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
